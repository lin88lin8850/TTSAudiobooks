<html>
  <head>
    <meta charset="UTF-8">	
    <title>Audio samples from "An End-to-End Role Recognition Model for Chinese Novel Audiobooks by Text-to-Speech"</title>
  </head>
  <body>

    <article>	
      <header>	
        <h1>Audiobook samples from "An end-to-end speaker determination model with joint learning for text-to-speech audiobooks"</h1>	
      </header>	
    </article>	

    <p><b>Authors: </b>Lin Wu, Junjie Pan, Xiang Yin, Zejun Ma.</p>	

    <p><b>Abstract:</b>
      Multi-role dubbing is critical for the fidelity of audiobooks generated by text-to-speech techniques. However, mass manual annotation of original novels with explicit speaker are required. In our previous work, a system with three modules was employed to determine the speaker of dialogues. Nevertheless, each module in the system was trained individually, which impeded the overall performance due to error propagation. In this paper, we propose an end-to-end speaker determination model with joint learning, which consists of a pre-trained language model, the speaker identification of dialogues module and the co-reference resolution module. Specifically, a pre-trained language model is applied to extract the context embedding for the speaker identification of dialogues module and the co-reference resolution module. In the learning process, the pre-trained language model and the two modules are jointly learned to extract the speaker corresponding to the dialogue automatically. By training our model in a teacher-forcing and end-to-end way, it can recognize the speaker of dialogues accurately. Compared to the previous approach, experiments show that our model achieves an absolute 9.46% improvement in objective speaker determination accuracy and 7.78% boost in subjective accuracy.</p>
    <h3> </h3>
    <HR align=center color="black" SIZE=1>

    <h3> Speaker tags in the TTS-based audiobooks examples provided by three methods </h3>
    <ul>
        <li>GT: Speaker tags given by manual annotation </li>
        <li>Baseline: Speaker tags predicted by the baseline model</li>
        <li>Ours: Speaker tags predicted by our end-to-end speaker determination model</li>
    </ul>
    <p><i> Audios were synthesized by a multi-speaker emotional TTS system. </i></p>
    <h3> </h3>
    <HR align=center color="black" SIZE=1>


    <!-- <h4> Demo 1 <a href="tmp/example.html" target="_blank">[Scripts]</a> </h4> -->

    <table>	
      <thead>	
        <tr>	
          <th>GT</th><th>Baseline</th><th>Ours</th>
        </tr>	
      </thead>	
      <tbody>	
        <tr><td colspan="6"><span>1: What do you usually do in the afternoon?</span></td></tr>	
        <tr>	
          <td><audio controls=""><source src="human/wav/6511678695453753870.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="baseline/wav/6511678695453753870.wav" type="audio/wav"></audio></td>	
          <td><audio controls=""><source src="ours/wav/6511678695453753870.wav" type="audio/wav"></audio></td>	
        </tr>	
        <tr><td colspan="6"><span>From online novel《最好的我们》written by 八月长安.</span></td></tr>	
      </tbody>	

  </body>	
</html>	