
<html>	
    <head>	
      <meta charset="UTF-8">	
      <title>Audio samples from "An End-to-End Role Recognition Model for Chinese Novel Audiobooks by Text-to-Speech"</title>
    </head>	
    <body>	
      <div>	
        <article>	
          <header>	
            <h1>Audio samples from "An End-to-End Role Recognition Model for Chinese Novel Audiobooks by Text-to-Speech"</h1>	
          </header>	
        </article>	
  
        <p><b>Authors: </b>Lin Wu, Junjie Pan, Xiang Yin, Zejun Ma.</p>	

        <p><b>Abstract:</b>
          Multi-role dubbing greatly improve performance in audiobooks made by text-to-speech techniques. The usual method of extracting all roles of the dialogues is to use a pipeline system. The system first uses a name entity recognition module to local all character word in the context, and then a speaker identification module is introduced to identify the speaker corresponding to the target dialogue, lastly a co-reference resolution module is used to get the role name of the identified speaker. However, the pipeline system cannot be trained in an end-to-end way so that the module error would propagate. In this paper, we propose an end-to-end dialogue role recognition model, which can extract the role corresponding to the dialogue automatically. The proposed model consists of a pre-trained language model and a dialogue text encoder module to locate the target speaker. A multi-task is introduced to find the co-references of the target speaker. By training the model in a teacher-forcing and end-to-end way, the model can recognize the speaker of target dialogue accurately. Compared to the baseline model, experiments show that our model achieve an absolutely 9.46% improvement in role recognition accuracy and the subjective role rationality rate is 7.78% higher.
        </p>
        <h3> </h3>
        <HR align=center color="black" SIZE=1>

        <!-- <h2>Controllability evaluation of speech synthesis backend</h2>	
        <p><i>Sample from Table 2 in the paper.</i></p>
        <p><b>Controllability evaluation: </b>under the given ground truth ToBI labels, test whether each ToBI feature is effectively reflected in generated speech.</p>
        <p><b>Pitch accents </b>mark the stressed syllable of specific words that carry the most information in a sentence. The default is H* (high accent), L* (low accent), L*+H (a syllable which starts with a low accent and then rises), L+H* (again low-high on one syllable, but with the second part accented), and !H*(This H is pitched somewhat lower than the earlier one).</p>
        <p><b>Boundary tones </b>describe the pitch trend at each full intonation phrase boundary. The default is H% (high tone) and L% (low tone).</p>
        <p><b>Phrase accents </b>describe the pitch movement between the ultimate pitch accent and the boundary tone. The default is H- and L-.</p>
        <table>		
          <tbody>	
            <tr><td colspan="6"><span>1a: Does Bob play basketball(H*) every(L*) day(L-H%)?</span></td></tr>	
            <tr>	
              <td><audio controls=""><source src="GT-ToBI/badcase_0087_1.wav" type="audio/wav"></audio></td>		
            </tr>
          </tbody>	
        </table> -->

        <!-- <h2>Comparison among systems</h2>	
        <p><i>Sample from Table 4 in the paper.</i></p>
        <p><b>TACO: </b>the model architecture is the same with the Tacotron and the input features are simply composed of the labels of phoneme, lexical stress, and word boundary.</p>
        <p><b>TP-DPE: </b>the unsupervised method. Phone-level prosodic features predicted from text, including phone duration, pitch and energy (DPE features), are used to realize prosody modeling.</p>
        <p><b>TP-ToBI: </b>the proposed method. the ToBI-related labels are predicted from text using the proposed ToBI prediction frontend.</p>
        <p><b>GT-ToBI: </b>the acoustic model and the input feature are the same as TP-ToBI, but the ToBI-related labels are manually revised from the results of ToBI prediction frontend.</p>
        <table>	
          <thead>	
            <tr>	
              <th>TACO</th><th>TP-DPE</th><th>TP-ToBI</th><th>GT-ToBI</th>
            </tr>	
          </thead>	
          <tbody>	
            <tr><td colspan="6"><span>1: What do you usually do in the afternoon?</span></td></tr>	
            <tr>	
              <td><audio controls=""><source src="TACO/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="TP-DPE/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="TP-ToBI/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="GT-ToBI/EN_testset_000022.wav" type="audio/wav"></audio></td>	
            </tr>
          </tbody>	 -->

      </div>	
    </body>	
  </html>	
  