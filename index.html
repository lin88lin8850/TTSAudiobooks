
<html>	
    <head>	
      <meta charset="UTF-8">	
      <title>Audio samples from "An End-to-End Role Recognition Model for Chinese Novel Audiobooks by Text-to-Speech"</title>
    </head>	
    <body>	
      <div>	
        <article>	
          <header>	
            <h1>Audio samples from "An End-to-End Role Recognition Model for Chinese Novel Audiobooks by Text-to-Speech"</h1>	
          </header>	
        </article>	
  
        <p><b>Authors: </b>Lin Wu, Junjie Pan, Xiang Yin, Zejun Ma.</p>	

        <p><b>Abstract:</b>
          Multi-role dubbing greatly improve performance in audiobooks made by text-to-speech techniques. The usual method of extracting all roles of the dialogues is to use a pipeline system. The system first uses a name entity recognition module to local all character word in the context, and then a speaker identification module is introduced to identify the speaker corresponding to the target dialogue, lastly a co-reference resolution module is used to get the role name of the identified speaker. However, the pipeline system cannot be trained in an end-to-end way so that the module error would propagate. In this paper, we propose an end-to-end dialogue role recognition model, which can extract the role corresponding to the dialogue automatically. The proposed model consists of a pre-trained language model and a dialogue text encoder module to locate the target speaker. A multi-task is introduced to find the co-references of the target speaker. By training the model in a teacher-forcing and end-to-end way, the model can recognize the speaker of target dialogue accurately. Compared to the baseline model, experiments show that our model achieve an absolutely 9.46% improvement in role recognition accuracy and the subjective role rationality rate is 7.78% higher.
        </p>
        <h3> </h3>
        <HR align=center color="black" SIZE=1>

        <h2> Roles in the TTS-based audiobooks examples provided by three systems </h2>
        <ul>
            <li>baseline: Roles predicted by the baseline system</li>
            <li>human: Roles given by manual annotation </li>
            <li>Ours-e2e: Roles predicted by our end-to-end role recognition system</li>
        </ul>
        <HR align=center color="black" SIZE=1>



        <!-- <h2>Comparison among systems</h2>	
        <p><i>Sample from Table 4 in the paper.</i></p>
        <p><b>TACO: </b>the model architecture is the same with the Tacotron and the input features are simply composed of the labels of phoneme, lexical stress, and word boundary.</p>
        <p><b>TP-DPE: </b>the unsupervised method. Phone-level prosodic features predicted from text, including phone duration, pitch and energy (DPE features), are used to realize prosody modeling.</p>
        <p><b>TP-ToBI: </b>the proposed method. the ToBI-related labels are predicted from text using the proposed ToBI prediction frontend.</p>
        <p><b>GT-ToBI: </b>the acoustic model and the input feature are the same as TP-ToBI, but the ToBI-related labels are manually revised from the results of ToBI prediction frontend.</p>
        <table>	
          <thead>	
            <tr>	
              <th>TACO</th><th>TP-DPE</th><th>TP-ToBI</th><th>GT-ToBI</th>
            </tr>	
          </thead>	
          <tbody>	
            <tr><td colspan="6"><span>1: What do you usually do in the afternoon?</span></td></tr>	
            <tr>	
              <td><audio controls=""><source src="TACO/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="TP-DPE/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="TP-ToBI/EN_testset_000022.wav" type="audio/wav"></audio></td>	
              <td><audio controls=""><source src="GT-ToBI/EN_testset_000022.wav" type="audio/wav"></audio></td>	
            </tr>
          </tbody>	 -->

      </div>	
    </body>	
  </html>	
  